# -*- coding: utf-8 -*-
"""NLP_Topic Modeling on Research Abstracts .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17dW8NGVfZQ950dnl3Xp9B6Y6CWWRx61G

# Business Understanding:
Decoding Science: Classifying Abstracts with NLP
~14,000 scientific abstracts

Each abstract is labeled with multiple research domains

Text is formal, technical, and information-dense

Task: Multi-label classification of abstracts

# Data Understanding
"""

import pandas as pd
train_path = "/content/Train.csv"
train_df = pd.read_csv(train_path)

print("Train Data:")
display(train_df.head())

# Dataset info
print("\nData Info:")
print(train_df.info())

# Shape (rows, columns)
print("\nShape:", train_df.shape)

# Size (total number of cells)
print("\nSize:", train_df.size)

# List of column names
print("\nColumn Names:", train_df.columns.tolist())

# Data types
print("\nData Types:")
print(train_df.dtypes)
# Unique values per column
print("\nUnique Values Per Column:")
print(train_df.nunique())

"""# Preprocess the Text Data

In this step, the abstracts were cleaned through a series of text processing techniques including lowercasing, removal of numbers and punctuation, tokenization, stopword removal, and lemmatization. This resulted in a new cleaned_abstract column ready for modeling. Exploratory analysis was also conducted using distribution plots and word clouds to better understand the text data.
"""

!pip install nltk --quiet

import pandas as pd
import numpy as np
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""Exploratory Data Analysis (EDA)"""

# Check null values
print("Null values:\n", train_df.isnull().sum())

# Length of each abstract
train_df['text_length'] = train_df['ABSTRACT'].astype(str).apply(len)

# Plot abstract length distribution
plt.figure(figsize=(10, 4))
plt.hist(train_df['text_length'], bins=50, color='skyblue')
plt.title('Distribution of Abstract Length')
plt.xlabel('Length of Abstract')
plt.ylabel('Frequency')
plt.show()

# Word cloud (Before cleaning)
text = " ".join(train_df['ABSTRACT'].dropna().astype(str).tolist())
wordcloud = WordCloud(width=1200, height=500, background_color='white').generate(text)

plt.figure(figsize=(14, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud of Research Abstracts (Before Cleaning)")
plt.show()

"""Preprocess the Text (Cleaning)
Scientific context	Numbers often carry important information like accuracy, model versions, physical constants, etc.
Semantic meaning	Terms like “2D CNN” or “BERT-Base-12” lose meaning if numbers are removed.
Classification relevance	Numbers may contribute to distinguishing subject areas, e.g., Physics vs AI.
Minimal noise	Unlike tweets or social media, your dataset is formal and technical, so numbers are not random or spammy.

"""

nltk.download('punkt_tab')
stop_words = set(stopwords.words('english'))  # Define stop_words here
lemmatizer = WordNetLemmatizer()  # Define lemmatizer here

def clean_text(text):
    text = text.lower()  # lowercase
    #text = re.sub(r"\d+", "", text)  # remove numbers
    text = re.sub(r"[^\w\s]", "", text)  # remove punctuation
    words = word_tokenize(text)  # tokenize
    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]  # remove stopwords + lemmatize
    return " ".join(words)

# Apply cleaning
train_df['cleaned_abstract'] = train_df['ABSTRACT'].astype(str).apply(clean_text)

# Display sample
train_df[['ABSTRACT', 'cleaned_abstract']].head()

"""Word Cloud (After Cleaning)"""

clean_text_all = " ".join(train_df['cleaned_abstract'].dropna().tolist())

wordcloud_clean = WordCloud(width=1200, height=500, background_color='black').generate(clean_text_all)

plt.figure(figsize=(14, 6))
plt.imshow(wordcloud_clean, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud of Cleaned Abstracts")
plt.show()

"""# Modeling & Evaluation:

why SciBERT?
Technical abstracts benefit immensely from context

BERT captures the nuances in scientific terminology

Better at generalizing rare or domain-specific terms

Improves macro F1, which matters in multi-label evaluation
"""

import pandas as pd
import numpy as np
import torch
from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from tqdm import tqdm

# Load dataset
df = pd.read_csv("Train.csv")
X = df['ABSTRACT']
y = df.iloc[:, 2:]

# Sample subset for speed
X_sample = X.sample(1000, random_state=42)
y_sample = y.loc[X_sample.index].reset_index(drop=True)

# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased").eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_model.to(device)

# Extract BERT [CLS] embeddings
def get_bert_embeddings(texts, tokenizer, model, device, max_len=128):
    embeddings = []
    with torch.no_grad():
        for text in tqdm(texts, desc="Extracting BERT embeddings"):
            encoded = tokenizer(text, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')
            input_ids = encoded['input_ids'].to(device)
            attention_mask = encoded['attention_mask'].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            cls = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()
            embeddings.append(cls)
    return np.array(embeddings)

X_bert = get_bert_embeddings(X_sample.tolist(), tokenizer, bert_model, device)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_bert, y_sample, test_size=0.2, random_state=42)

# Build neural network
model = Sequential([
    Input(shape=(768,)),
    Dense(512, activation='relu'),
    Dense(y_train.shape[1], activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer=Adam(1e-4), metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# Predict and evaluate
y_pred = model.predict(X_test)
y_pred_binary = (y_pred >= 0.5).astype(int)

f1_micro = f1_score(y_test, y_pred_binary, average='micro')
f1_macro = f1_score(y_test, y_pred_binary, average='macro')

print(f"Micro F1-score: {f1_micro:.4f}")
print(f"Macro F1-score: {f1_macro:.4f}")

!pip install transformers torch scikit-learn tensorflow matplotlib
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input, Dropout
from tensorflow.keras.optimizers import Adam
from tqdm import tqdm
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("Train.csv")
X = df['ABSTRACT']
y = df.iloc[:, 2:]

# Sample subset for speed
X_sample = X.sample(1000, random_state=42)
y_sample = y.loc[X_sample.index].reset_index(drop=True)

# Load SciBERT model
tokenizer = AutoTokenizer.from_pretrained("allenai/scibert_scivocab_uncased")
scibert_model = AutoModel.from_pretrained("allenai/scibert_scivocab_uncased").eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
scibert_model.to(device)

# Extract embeddings
def get_scibert_embeddings(texts, tokenizer, model, device, max_len=128):
    embeddings = []
    with torch.no_grad():
        for text in tqdm(texts, desc="Extracting SciBERT embeddings"):
            encoded = tokenizer(text, padding='max_length', truncation=True, max_length=max_len, return_tensors='pt')
            input_ids = encoded['input_ids'].to(device)
            attention_mask = encoded['attention_mask'].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            cls = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()
            embeddings.append(cls)
    return np.array(embeddings)

X_bert = get_scibert_embeddings(X_sample.tolist(), tokenizer, scibert_model, device)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_bert, y_sample, test_size=0.2, random_state=42)

# Build deeper NN model
model = Sequential([
    Input(shape=(768,)),
    Dense(512, activation='relu'),
    Dropout(0.3),
    Dense(256, activation='relu'),
    Dropout(0.2),
    Dense(y_train.shape[1], activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer=Adam(1e-4), metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# Predict probabilities
y_pred_probs = model.predict(X_test)

# Tune threshold
best_f1, best_thresh = 0, 0.5
for t in np.arange(0.1, 0.9, 0.05):
    y_pred = (y_pred_probs >= t).astype(int)
    score = f1_score(y_test, y_pred, average='micro')
    if score > best_f1:
        best_f1, best_thresh = score, t

# Final prediction with best threshold
y_pred_final = (y_pred_probs >= best_thresh).astype(int)
f1_micro = f1_score(y_test, y_pred_final, average='micro')
f1_macro = f1_score(y_test, y_pred_final, average='macro')

print(f"Best threshold: {best_thresh}")
print(f"Micro F1-score: {f1_micro:.4f}")
print(f"Macro F1-score: {f1_macro:.4f}")

# Plot training loss
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Binary Crossentropy Loss')
plt.legend()
plt.grid(True)
plt.savefig("training_loss_plot.png")
plt.show()

"""“Using SciBERT embeddings, a deeper neural network, and dynamic thresholding significantly improved multi-label classification performance over standard BERT embeddings, particularly in detecting rare scientific topics (macro F1 ↑ from 0.11 to 0.34).”                                 
SciBERT outperforms regular BERT by a significant margin in both Micro and Macro F1.

Micro F1 ↑ by ~15% — overall prediction quality improved.

Macro F1 ↑ by ~29% — SciBERT handles rare labels much better.

Threshold tuning matters: Instead of the default 0.5, a lower threshold (~0.2) worked best. This is common for imbalanced multi-label problems.

Model generalizes better: Your val_loss is low and stable, suggesting no overfitting — dropout helped!
"""

example_text = "We introduce a novel approach for solving partial differential equations using neural operators and variational methods."
encoded = tokenizer(example_text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)
input_ids = encoded['input_ids'].to(device)
attention_mask = encoded['attention_mask'].to(device)

with torch.no_grad():
    output = scibert_model(input_ids=input_ids, attention_mask=attention_mask)
    example_embedding = output.last_hidden_state[:, 0, :].cpu().numpy()
y_pred_probs = model.predict(example_embedding)
y_pred_binary = (y_pred_probs >= best_thresh).astype(int)
predicted_labels = y.columns[y_pred_binary[0] == 1].tolist()
print("Predicted Labels:", predicted_labels)

"""# Deployment"""

model.save("scibert_multilabel_model.keras")

from tensorflow.keras.models import load_model
model = load_model("scibert_multilabel_model.keras")